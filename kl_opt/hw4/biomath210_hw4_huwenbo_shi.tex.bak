\documentclass{scrartcl}
\usepackage{etoolbox}
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathabx}
\usepackage{graphicx}
\usepackage{float}
\usepackage{parskip}
\usepackage{indentfirst}
\usepackage{fancyhdr}
\usepackage{hyperref}
\pagestyle{fancy}
\usepackage{subcaption}
\setlength{\parskip}{0em}
\setlength{\parindent}{2em}

%-----------------------------------------------------------------------------
\begin{document}





%-----------------------------------------------------------------------------
% header
\lhead{Huwenbo Shi (603-778-363) shihuwenbo@ucla.edu}

% title
\newcommand*{\TitleFont}{
      \usefont{\encodingdefault}{\rmdefault}{b}{n}
      \fontsize{16}{20}
      \selectfont}
\newcommand*{\AuthorFont}{
      \usefont{\encodingdefault}{\rmdefault}{r}{n}
      \fontsize{12}{20}
      \selectfont}
\title{\TitleFont Biomath 210 Homework 4}
\author{\AuthorFont Huwenbo Shi (603-778-363) shihuwenbo@ucla.edu}
\maketitle

\newcommand*{\argmin}{\operatornamewithlimits{argmin}\limits}
\newcommand*{\argmax}{\operatornamewithlimits{argmax}\limits}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\dom}{\mathrm{dom}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\epi}{\mathrm{epi}}
\def\mb#1{\mathbf{#1}}

%-----------------------------------------------------------------------------

\section*{Problem 5.6}

We assume $t > 0$, $\max_i x_i$


\section*{Problem 5.8}

%Let $g(x) = -czx + c \ln (1+e^x) + {1 \over 2} | y - x |^2$, then
%\begin{equation}
%	{ d g(x) \over d x } = -cz + { c e^x \over {1 + e^x} } + x - y , \;\;
%	{ d^2 g(x) \over d x^2 } = { c e^x \over {(1 + e^x)^2} } + 1	.
%\end{equation}
Matlab script implementing Newton's method for finding $\text{prox}_{cf}(y)$ is attached.
Figure \ref{fig:prob8_result} shows some results for different initializations of $y$, $c$, and $z$.
First 3 iterations of Newton's method is marked on the plot of the function.

\begin{verbatim}

% initialization
y = 2.0; c = 2.0; x = -5.0; z = 0.0; r = 0.9;
eps = 10^-8; max_iter = 100;

% find prox using newton's method
for i=1:max_iter
    
    % compute first and second derivative
    derv1 = -c*z + c*exp(x)/(1+exp(x)) + x - y;
    derv2 = c*exp(x)/(1+exp(x))^2 + 1;

    % backtracking line search
    t = 1; next_x = x - t*derv1/derv2;
    cur_obj = -c*z*x + c*log(1+exp(x)) + 0.5*(y-x)^2;
    while(cur_obj < -c*z*next_x + c*log(1+exp(next_x)) + 0.5*(y-next_x)^2)
        t = t*r; next_x = x - t*derv1/derv2;
    end
    
    % check stop condition
    new_x = x - t*derv1/derv2;
    new_obj = -c*z*new_x + c*log(1+exp(new_x)) + 0.5*(y-new_x)^2;
    if(cur_obj - new_obj < eps || abs(new_x-x) < eps), break; end
    
    % update x
    x = new_x;
    
end

\end{verbatim}

\begin{figure}[H]
  \begin{minipage}[b]{0.3\textwidth}
    \centering
    \includegraphics[scale=0.26]{prob_8_y_1_c_1_z_1.eps}
  \end{minipage}
  \quad
  \begin{minipage}[b]{0.3\textwidth}
    \centering
    \includegraphics[scale=0.26]{prob_8_y_2_c_1_z_1.eps}
  \end{minipage}
  \quad
  \begin{minipage}[b]{0.3\textwidth}
    \centering
    \includegraphics[scale=0.26]{prob_8_y_3_c_1_z_1.eps}
  \end{minipage}
  
  \begin{minipage}[b]{0.3\textwidth}
    \centering
    \includegraphics[scale=0.26]{prob_8_y_2_c_2_z_0.eps}
  \end{minipage}
  \quad
  \begin{minipage}[b]{0.3\textwidth}
    \centering
    \includegraphics[scale=0.26]{prob_8_y_2_c_2_z_1.eps}
  \end{minipage}
  \quad  
  \begin{minipage}[b]{0.3\textwidth}
    \centering
    \includegraphics[scale=0.26]{prob_8_y_2_c_3_z_1.eps}
  \end{minipage}
  \caption{$\text{prox}_{cf}(y)$ for different initializations of $y$, $c$, and $z$}
  \label{fig:prob8_result}
\end{figure}


\section*{Problem 5.12}

To find the projection onto $S$, we solve the problem of minimizing
${1 \over 2}\|\mb{x}-\mb{y}\|^2$ subject to the constraint
$\mb{1}^*\mb{x} = 0$ and $\mb{x}^*\mb{x} = p$. Because
${1 \over 2}\|\mb{x}-\mb{y}\|^2 = {1 \over 2}(\|\mb{x}\|^2 + \|\mb{y}\|^2) - \mb{y}^*\mb{x}$,
for $\|\mb{x}\|^2$ constrained at $p$, the problem is equivalent to minimizing the
linear term $-\mb{y}^*\mb{x}$ with the same constraint. The Lagrangian of the transformed problem is
\begin{equation}
\mathcal{L}(\mb{x}, \lambda_1, \lambda_2)
= -\mb{y}^*\mb{x} + \lambda_1 \mb{1}^*\mb{x} + \lambda_2 \mb{x}^*\mb{x} - \lambda_2 p.
\end{equation}
Setting the gradient of the Lagrangian to 0, we have
\begin{equation}
0 = -\mb{y}\ + \lambda_1 \mb{1} + 2 \lambda_2 \mb{x} , \; \text{and} \;
\mb{x} = { {\mb{y} - \lambda_1 \mb{1}} \over {2 \lambda_2} }.
\label{eqn:x_512}
\end{equation}
Because of the constraint $\mb{1}^*\mb{x} = 0$, we have
\begin{equation}
{ { \mb{1}^*({\mb{y} - \lambda_1 \mb{1}}) } \over {2 \lambda_2} } = 0 ,
\end{equation}
which implies
\begin{equation}
\lambda_1 = {1 \over p} \mb{1}^*\mb{y} =  \bar{y}.
\end{equation}
Substituting $\lambda_1$ into Equation \eqref{eqn:x_512} gives
\begin{equation}
\mb{x} = { {\mb{y} - \bar{y} \mb{1}} \over {2 \lambda_2} } .
\label{eqn:x_2_512}
\end{equation}
The constraint $\mb{x}^*\mb{x} = p$ entails
\begin{equation}
{ \|{\mb{y} - \bar{y} \mb{1}}\|^2 \over {4 \lambda_2^2} } = p, \; \text{and} \;
\lambda_2^2 = { {\|{\mb{y} - \bar{y} \mb{1}}\|^2} \over {4p} }.
\end{equation}
Taking $\lambda_2 = { {\|{\mb{y} - \bar{y} \mb{1}}\|} \over {2 \sqrt{p}} }$, and substituting
$\lambda_2$ into Equation \eqref{eqn:x_2_512} gives the projection
\begin{equation}
 P_S(\mb{y}) = {\sqrt{p} \over {\|\mb{y}-\bar{y}\mb{1}\|}}(\mb{y}-\bar{y}\mb{1}).
\end{equation}

When $\mb{y} = \bar{y}\mb{1}$, for any $\mb{x} \in S$ (i.e. $\mb{x}^*\mb{x}=p$ and $\mb{1}^*\mb{x} = 0$),
\begin{equation}
\|\mb{x}-\bar{y}\mb{1}\|^2 = \|\mb{x}\|^2 + \|\bar{y}\mb{1}\|^2-2\mb{x}^*(\bar{y}\mb{1})
= p + \bar{y}^2 p,
\end{equation}
therefore, all points of $S$ are equidistant from $\mb{y}$.

\section*{Problem 5.13}

Finding the projection $P_S(\mb{y})$ is equivalent to solve the problem
$\min_{\mb{x}} \| \mb{y} - \mb{x} \|^2$, with the constraints $\mb{x}^*\mb{x} = 1$
and $\mb{x} \ge 0$. We notice that
\begin{equation}
\| \mb{y} - \mb{x} \|^2 = \|\mb{x}\|^2 + \|\mb{y}\|^2 - 2\mb{x}^*\mb{y}
= 1 + \|\mb{y}\|^2 -2 \mb{x}^*\mb{y} ,
\end{equation}
where the second equality follows from the constraint $\mb{x}^*\mb{x} = 1$.
For a fixed $\mb{y}$, finding the projection is then equivalent to solving
$\min_{\mb{x}}-\mb{y}^*\mb{x}$, with the constraint that $\mb{x} \in S$.
The Lagrangian of the problem is
\begin{equation}
    \mathcal{L}(\mb{x}, \lambda, \pmb{\mu})
    = -\mb{y}^*\mb{x} + \lambda \mb{x}^*\mb{x} - \lambda - \pmb{\mu}^*\mb{x} .
\end{equation}
The KKT condition entails the following equalities at a stationary point
\begin{equation}
0 = -y_i + 2\lambda x_i - \mu_i, \; \mu_i \ge 0, \; \mu_i x_i = 0 \; \forall i ,
\end{equation}
which then entails
\begin{equation}
    x_i = {1 \over {2\lambda}} (\mu_i + y_i).
    \label{eqn:xi_513}
\end{equation}
From the constraint $\mb{x}^*\mb{x} = 1$, we have
\begin{equation}
    \sum_{i=1}^p x_i^2 = {1 \over 4\lambda^2} \sum_{i=1}^p (\mu_i + y_i)^2 = 1,
    \text{and \;} \lambda^2 = {\sum_{i=1}^p (\mu_i + y_i)^2 \over 4}.
\end{equation}
From complementary slackness, we also have
\begin{equation}
    x_i = {1 \over 2\lambda} y_i, \; \mu_i = 0, \; \text{if\;} x_i > 0.
    \label{eqn:cs_513}
\end{equation}
%\begin{equation}
%    x_i = \begin{cases}
%            {1 \over 2\lambda} y_i & \text{if\;} x_i > 0 \\
%            {1 \over 2\lambda} (\mu_i + y_i) & \text{if\;} x_i = 0.
%          \end{cases}
%    \label{eqn:cs_513}
%\end{equation}

\textbf{**When $y_i < 0$ for all $i$}:

When $y_i < 0$ for all $i$, we must have $\lambda < 0$
(Equation \eqref{eqn:cs_513}) and $0 \le \mu_i \le -y_i$ (Equation \eqref{eqn:xi_513})
in order for the constraint $\mb{x} \ge 0$ to hold, which entails
\begin{equation}
    \lambda = -{\sqrt{\sum_{i=1}^p {(\mu_i + y_i)^2}} \over 2}, \; \text{and} \;
    x_i = -{\mu_i + y_i \over {\sqrt{\sum_{i=1}^p {(\mu_i + y_i)^2}}}} .
    \label{eqn:lambda_513}
\end{equation}
From complementary slackness, if $x_i = 0$, we must have $\mu_i = -y_i$, if $x_i > 0$,
we must have $\mu_i = 0$. Therefore,
\begin{equation}
    x_i = \begin{cases}
            -{y_i \over {\sqrt{\sum_{\{i:x_i > 0\}} {y_i^2}}}} & \text{if} \; x_i > 0 \\
            0 & \text{otherwise}
          \end{cases}
\end{equation}
Let $C = \{i : x_i > 0\}$, then
\begin{equation}
    -\mb{y}^*\mb{x}
    = \sum_{i \in C} {y_i^2 \over {\sqrt{\sum_{i \in C} {y_i^2}}}}
    = \sqrt{\sum_{i \in C} {y_i^2}}.
\end{equation}
Clearly, by setting $C = \{i : i = \argmin_j y_j^2\}$, minimizes $-\mb{y}^*\mb{x}$.
To satisfy the constraint $\mb{x}^*\mb{x} = 1$, we must have $x_i = 1$ at $i = \argmin_j -y_j$,
and 0 else where. Therefore, the projection $P_S(\mb{y}) = \mb{e}_i$, where $y_i$ is the least negative.

\textbf{**When $y_i > 0$ for some $i$}:

When $y_i > 0$ for some $i$, we investigate the case $y_i > 0$, $y_i = 0$, and $y_i < 0$ separately.
\begin{itemize}
\item For the case $y_i > 0$, we have $x_i = {1 \over {2\lambda}} (\mu_i + y_i)$. Because $\mu_i \ge 0$,
we must have $\lambda > 0$ and $x_i > 0$. From complementary slackness $x_i\mu_i = 0$,
so we must have $\mu_i = 0$. Therefore, for $y_i > 0$, $x_i = {1 \over {2\lambda}} y_i$.
\item For $y_i = 0$, we have $x_i = {1 \over {2\lambda}} \mu_i$, and from complementary slackness,
we have $x_i = \mu_i = 0$. For $y_i < 0$, we have $x_i = {1 \over {2\lambda}} (\mu_i + y_i)$.
\item For $y_i < 0$, we have $x_i = {1 \over {2\lambda}} (\mu_i + y_i)$.
If $x_i = 0$, then $0 = {1 \over {2\lambda}} (\mu_i + y_i)$ entails
$\mu_i = -y_i$. If $x_i > 0$, then $\mu_i$ must be equal to 0, which entails 
$x_i = {1 \over {2\lambda}} y_i$, contradicting $x_i > 0$. Therefore, for $y_i < 0$, $x_i = 0$.
\end{itemize}
From the constraint $\mb{x}^*\mb{x} = 1$, we conclude that
$\lambda = {\sqrt{\sum_{\{i:y_i > 0\}}  y_i^2} \over 2}$. To summarize, when $y_i > 0$ for some $i$
\begin{equation}
x_i = \begin{cases}
        {y_i \over \sqrt{\sum_{\{i:y_i > 0\}}  y_i^2}} & \text{if} \; y_i > 0 \\
        0 & \text{if} \; y_i \le 0 .
      \end{cases}
\end{equation}
So $P_{S}(\mb{y})$ is obtained by setting the non-positive part to 0, and then projecting the rest
onto a lower-dimensional unit sphere.

\textbf{**Cases where $P_S(\mb{y})$ is multivalued}:
\begin{itemize}
\item When $y_i < 0$ for all $i$, if there are multiple $y_i$'s that have the same value
\item When $y_i < 0$ for some $i$, and all other $y_i = 0$
\item If $y_i = 0$ for all $i$
\end{itemize}


\section*{Problem 5.14}

Let $S = \{\mb{e}_1, \dotsc, \mb{e}_p\}$, and we want to prove that
\begin{equation}
P_S(\mb{y}) = \argmin_{\mb{x} \in S} \|\mb{x}-\mb{y}\| = \mb{e}_i ,
\end{equation}
where $i = \argmax_{j \in \{1,\dotsc,p\}} y_j$.

To prove this, we suppose that $y_k = \max_{j \in \{1,\dotsc,p\}} y_j$, but $P_S(\mb{y}) = \mb{e}_l$
for some $y_l < y_k$. We notice that
\begin{equation}
    \begin{split}
    \|\mb{y} - \mb{e}_l\|^2 & = \sum_{i=1, i\neq k, i\neq l} y_i^2 + y_k^2 + (y_l-1)^2 \\
    \|\mb{y} - \mb{e}_k\|^2 & = \sum_{i=1, i\neq k, i\neq l} y_i^2 + (y_k-1)^2 + y_l^2 ,
    \end{split}
\end{equation}
and that
\begin{equation}
\begin{split}
\|\mb{y} - \mb{e}_k\|^2 - \|\mb{y} - \mb{e}_l\|^2 & = 2(y_l-y_k) < 0 \\
\|\mb{y} - \mb{e}_k\| & < \|\mb{y} - \mb{e}_l\|,
\end{split}
\end{equation}
contradicting the assumption that $\mb{e}_l$ is the projection of $\mb{y}$ onto $S$.
Therefore, the projection $P_S(\mb{y})$ must be $\mb{e_k}$, at which we have
$\|\mb{y} - \mb{e}_k\| \le \|\mb{y} - \mb{e}_l\|$ for all $l$, i.e. $\mb{e}_k$
minimizes $\|\mb{x}-\mb{y}\|$ with the constraint that $\mb{x} \in S$.


\section*{Problem 5.15}

Let $S = \{\mb{N} : \mb{N} \in \mathbf{R}^{p \times q}, q \le p, \mb{N}^*\mb{N} = \mb{I}_q\}$. The projection $P_{S}(\mb{M})$
can be found by minimizing ${1 \over 2} \|\mb{M} - \mb{N}\|_F^2$ subject to the constraint $\mb{N}^*\mb{N} = \mb{I}_q$.
Let $\mb{M} = \mb{U}\mb{D}\mb{V}^*$ be the singular value decomposition of $\mb{M}$, where $\mb{U} \in \mb{R}^{p \times q}$
and $\mb{V} \in \mb{R}^{q \times q}$ are orthogonal and $\mb{D} \in \mb{R}^{q \times q}$ diagonal. Because Frobenius norm is orthogonal invariant,
it can be shown that $\|\mb{M}-\mb{N}\|_F^2 = \|\mb{D}-\mb{U}^*\mb{N}\mb{V}\|_F^2$. Then it follows that
\begin{equation}
\begin{split}
{1 \over 2} \|\mb{M} - \mb{N}\|_F^2 
& = {1 \over 2} \|\mb{D}-\mb{U}^*\mb{N}\mb{V}\|_F^2 \\
& = {1 \over 2} \tr [ (\mb{D}-\mb{U}^*\mb{N}\mb{V})^*(\mb{D}-\mb{U}^*\mb{N}\mb{V}) ] \\
& = {1 \over 2} \tr [\mb{D}^*\mb{D} + \mb{I}_q] - \tr[\mb{D}^*\mb{U}^*\mb{N}\mb{V}] .
\end{split}
\end{equation}
Therefore, the projection problem is equivalent to minimizing $-\tr[\mb{D}^*\mb{U}^*\mb{N}\mb{V}]$,
with the constraint $\mb{N}^*\mb{N} = \mb{I}_q$. The Lagrangian of this problem is
\begin{equation}
\mathcal{L}(\mb{N}, \pmb{\Lambda})
= -\tr[\mb{D}^*\mb{U}^*\mb{N}\mb{V}] + \tr[\pmb{\Lambda} \mb{N}^*\mb{N} - \pmb{\Lambda}\mb{I}_q],
\end{equation}
where $\pmb{\Lambda}$ is the symmetric matrix of Lagrange multipliers.
Setting the gradient of the Lagrangian to 0, we get
\begin{equation}
\begin{split}
0 & = -\mb{U} \mb{D} \mb{V}^* + \mb{N} (\pmb{\Lambda} + \pmb{\Lambda}^*)
  = -\mb{U} \mb{D} \mb{V}^* + 2 \pmb{\Lambda} \mb{N} \\
\mb{N} & = {1 \over 2} \pmb{\Lambda}^{-1} \mb{U} \mb{D} \mb{V}^*.
\end{split}
\end{equation}
The constraint $\mb{N}^*\mb{N}$ entails
\begin{equation}
{1 \over 4} (\mb{V} \mb{D} \mb{U}^* \pmb{\Lambda}^{-2} \mb{U} \mb{D} \mb{V}^*) = \mb{I}_q.
\end{equation}
The solution $\pmb{\Lambda}^{-1} = 2 \mb{U} \mb{D}^{-1} \mb{U}^*$ clearly satisfies the equality above, which gives
\begin{equation}
\mb{N} = {1 \over 2} \pmb{\Lambda}^{-1} \mb{U} \mb{D} \mb{V}^* 
= \mb{U} \mb{D}^{-1} \mb{U}^* \mb{U} \mb{D} \mb{V}^* = \mb{U} \mb{V}^*.
\end{equation}


\section*{Problem 5.26}

Because $f(\mb{x})$ is a convex function, it must satisfy the supporting hyperplane inequality
$f(\mb{x}) \ge f(\mb{x}) + \mb{g}^*(\mb{x}-\mb{y})$, where $\mb{g}$ is the subgradient of $f(\mb{x})$ at $\mb{x}$.
(On the other hand, if $f(\mb{x})$ satisfies the supporting hyperplane inequality, it's convex.) Therefore, at $\mb{x}_i$ and $\mb{x}_j$,
the following inequality must be satisfied for $f(\mb{x})$ to be convex
\begin{equation}
    f(\mb{x}_j) \ge f(\mb{x}_i)  + \mb{g}_j^*(\mb{x}_j-\mb{x}_i),
\end{equation}
where $\mb{g}_j$ denotes the subgradient of $f(\mb{x})$ at $\mb{x}_j$.
Let $z_i = f(\mb{x}_i)$, the problem of finding a convex function $f(\mb{x})$ that minimizes the sum of squares $\sum_{i=1}^n [y_i-f(\mb{x}_i)]^2$,
is therefore equivalent to solving the problem of minimizing $\sum_{i=1}^n [y_i-z_i]^2$, with the constraints
$z_j \ge z_i  + \mb{g}_j^*(\mb{x}_j-\mb{x}_i)$. In this problem, the variables that need to be found are $z_i$, and $g_i$.
Because the objective of this problem is quadratic, and the constraints are linear, it can be solved using convex optimization techniques.7

\section*{Problem 5.27}

To prove majorization, we notice that
\begin{equation}
 {1 \over 2} \|\mb{D}\|^2 \|\mb{x}-\mb{x}_n\|^2 \ge {1 \over 2} \|\mb{D}\ (\mb{x}-\mb{x}_n) \|^2
 = {1 \over 2} \mb{x}_n^* \mb{D}^* \mb{D} \mb{x}_n + {1 \over 2} \mb{x}^* \mb{D}^* \mb{D} \mb{x} - \mb{x}_n^* \mb{D}^* \mb{D} \mb{x} ,
\end{equation}
which entails
\begin{equation}
\begin{split}
& {1 \over 2} \|\mb{D}\mb{x}_n\|^2 + \mb{x}_n\mb{D}^*\mb{D}(\mb{x}-\mb{x}_n) +  {1 \over 2} \|\mb{D}\|^2 \|\mb{x}-\mb{x}_n\|^2 \\
& \ge {1 \over 2} \|\mb{D}\mb{x}_n\|^2 + \mb{x}_n\mb{D}^*\mb{D}(\mb{x}-\mb{x}_n)  + {1 \over 2} \|\mb{D}\ (\mb{x}-\mb{x}_n) \|^2 \\
& = {1 \over 2} \mb{x}_n^* \mb{D}^* \mb{D} \mb{x}_n +  \mb{x}_n\mb{D}^*\mb{D}\mb{x} - \mb{x}_n\mb{D}^*\mb{D}\mb{x}_n + 
{1 \over 2} \mb{x}_n^* \mb{D}^* \mb{D} \mb{x}_n + {1 \over 2} \mb{x}^* \mb{D}^* \mb{D} \mb{x} - \mb{x}_n^* \mb{D}^* \mb{D} \mb{x} \\
& = {1 \over 2} \mb{x}^* \mb{D}^* \mb{D} \mb{x} = {1 \over 2} \| \mb{D} \mb{x} \|^2.
\end{split}
\end{equation}
Problem 18 of chapter 1, in which $\mb{D} = (1, 1, -1)$, is a special case of this problem.


\end{document}